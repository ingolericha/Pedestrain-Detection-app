# -*- coding: utf-8 -*-
"""Copy of CV_Pedestrian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xoKCEPPk6rnWII_st13cqyb3RCLpuV0B
"""

#@title HOG Model
import cv2
import imutils
from google.colab.patches import cv2_imshow  # For displaying images in Colab
from google.colab import files  # For uploading files

def pedestrian_detection_hog(image, hog):
    # Resize the image for consistent detection performance
    image = imutils.resize(image, width=700)
    (rects, weights) = hog.detectMultiScale(image, winStride=(4, 4),
                                            padding=(8, 8), scale=1.05)

    # Apply non-maxima suppression to reduce overlapping boxes
    rects = cv2.groupRectangles(rects.tolist(), groupThreshold=1, eps=0.5)[0] if len(rects) > 0 else []

    return rects

# Initialize the HOG descriptor/person detector
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())

# Upload the video to Colab
print("Please upload the input video file.")
uploaded = files.upload()
input_video_path = list(uploaded.keys())[0]

# Open the video file
cap = cv2.VideoCapture(input_video_path)

# Process the video
frame_count = 0

while True:
    # Read a frame from the video
    (grabbed, frame) = cap.read()
    if not grabbed:
        break

    # Detect pedestrians in the frame
    boxes = pedestrian_detection_hog(frame, hog)

    # Draw the bounding boxes around detected pedestrians
    for (x, y, w, h) in boxes:
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

    # Display the processed frame
    cv2_imshow(frame)

    frame_count += 1
    if frame_count >= 200:  # Display first 200 frames for performance
        break

cap.release()

#@title YOLO
import numpy as np
import cv2
import os
import imutils
from google.colab.patches import cv2_imshow  # To display images in Colab
from google.colab import files  # To upload/download files

NMS_THRESHOLD = 0.3
MIN_CONFIDENCE = 0.2

def pedestrian_detection(image, model, layer_name, personidz=0):
    (H, W) = image.shape[:2]
    results = []

    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),
                                 swapRB=True, crop=False)
    model.setInput(blob)
    layerOutputs = model.forward(layer_name)

    boxes = []
    centroids = []
    confidences = []

    for output in layerOutputs:
        for detection in output:
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            if classID == personidz and confidence > MIN_CONFIDENCE:
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                boxes.append([x, y, int(width), int(height)])
                centroids.append((centerX, centerY))
                confidences.append(float(confidence))

    idzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)

    if len(idzs) > 0:
        for i in idzs.flatten():
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            res = (confidences[i], (x, y, x + w, y + h), centroids[i])
            results.append(res)

    return results

# Load YOLO model
labelsPath = "/content/coco.names"
LABELS = open(labelsPath).read().strip().split("\n")
weights_path = "yolov4-tiny.weights"
config_path = "yolov4-tiny.cfg"

model = cv2.dnn.readNetFromDarknet(config_path, weights_path)

# Fix for the layer_name indexing issue
layer_name = model.getLayerNames()
try:
    # In some versions of OpenCV, getUnconnectedOutLayers returns a list of scalars
    layer_name = [layer_name[i[0] - 1] for i in model.getUnconnectedOutLayers()]
except:
    # If getUnconnectedOutLayers returns a list of integers
    layer_name = [layer_name[i - 1] for i in model.getUnconnectedOutLayers()]

# Upload the video to Colab
print("Please upload the input video file.")
uploaded = files.upload()
input_video_path = list(uploaded.keys())[0]

cap = cv2.VideoCapture(input_video_path)

# For output video writing
output_video_path = "/content/output_pedestrian_detection.avi"
writer = None

while True:
    (grabbed, image) = cap.read()
    if not grabbed:
        break

    image = imutils.resize(image, width=700)
    results = pedestrian_detection(image, model, layer_name, personidz=LABELS.index("person"))

    # Draw bounding boxes on the image
    for res in results:
        cv2.rectangle(image, (res[1][0], res[1][1]), (res[1][2], res[1][3]), (0, 255, 0), 2)

    # Initialize the video writer if needed
    if writer is None:
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")
        writer = cv2.VideoWriter(output_video_path, fourcc, 30,
                                 (image.shape[1], image.shape[0]), True)

    # Write the frame to the output video
    writer.write(image)

    # Display frame in Colab (you can remove this in case of large videos)
    cv2_imshow(image)

cap.release()
writer.release()

# Download the processed output video
print("Downloading the output video...")
files.download(output_video_path)

# Install required libraries
!pip install streamlit pyngrok opencv-python-headless imutils -q

# Re-authenticate ngrok (replace with your own token)
!ngrok authtoken 2pFNFYLaFM3EPoGHLeXjQZGfAsO_6zNPz3hMXPECBRhaw3XSf

# Import required libraries
import numpy as np
import cv2
import imutils
from pyngrok import ngrok

# Streamlit app code
streamlit_code = """
import streamlit as st
import numpy as np
import cv2
import imutils

# YOLO detection settings
NMS_THRESHOLD = 0.3
MIN_CONFIDENCE = 0.2

def pedestrian_detection(image, model, layer_name, personidz=0):
    (H, W) = image.shape[:2]
    results = []

    # Prepare the image for YOLO
    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),
                                 swapRB=True, crop=False)
    model.setInput(blob)
    layerOutputs = model.forward(layer_name)

    # Initialize variables for detection
    boxes = []
    centroids = []
    confidences = []

    for output in layerOutputs:
        for detection in output:
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            if classID == personidz and confidence > MIN_CONFIDENCE:
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                boxes.append([x, y, int(width), int(height)])
                centroids.append((centerX, centerY))
                confidences.append(float(confidence))

    # Apply non-maxima suppression
    idzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)

    if len(idzs) > 0:
        for i in idzs.flatten():
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])
            res = (confidences[i], (x, y, x + w, y + h), centroids[i])
            results.append(res)

    return results

# Load YOLO model
labelsPath = "coco.names"
weights_path = "yolov4-tiny.weights"
config_path = "yolov4-tiny.cfg"

model = cv2.dnn.readNetFromDarknet(config_path, weights_path)
layer_name = model.getLayerNames()
layer_name = [layer_name[i - 1] for i in model.getUnconnectedOutLayers()]

# Streamlit app UI
st.title("Pedestrian Detection App")
st.write("Upload a video to detect pedestrians.")

uploaded_file = st.file_uploader("Choose a video...", type=["mp4", "avi", "mov"])

if uploaded_file is not None:
    # Save uploaded file
    video_path = "uploaded_video.mp4"
    with open(video_path, "wb") as f:
        f.write(uploaded_file.read())

    cap = cv2.VideoCapture(video_path)
    frame_count = 0

    stframe = st.empty()  # For real-time video display

    while True:
        grabbed, frame = cap.read()
        if not grabbed:
            break

        frame = imutils.resize(frame, width=700)
        results = pedestrian_detection(frame, model, layer_name, personidz=0)

        for res in results:
            (startX, startY, endX, endY) = res[1]
            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)

        # Convert frame to RGB for Streamlit
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        stframe.image(frame, channels="RGB", use_container_width=True)

        frame_count += 1
        if frame_count >= 200:  # Limit to 200 frames for performance
            break

    cap.release()
    st.write("Video processing complete!")
"""

# Save the Streamlit code to a Python file
with open("app.py", "w") as f:
    f.write(streamlit_code)

# Start ngrok to expose the Streamlit app
public_url = ngrok.connect("8501")
print(f"Streamlit app is live at {public_url}")

# Run the Streamlit app
!streamlit run app.py &>/dev/null &

# Import required libraries
import streamlit as st
import numpy as np
import cv2
import imutils
import os
from pyngrok import ngrok

# Constants
NMS_THRESHOLD = 0.3
MIN_CONFIDENCE = 0.2

def pedestrian_detection(image, model, layer_name, personidz=0):
    (H, W) = image.shape[:2]
    results = []

    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    model.setInput(blob)
    layerOutputs = model.forward(layer_name)

    boxes = []
    centroids = []
    confidences = []

    for output in layerOutputs:
        for detection in output:
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            if classID == personidz and confidence > MIN_CONFIDENCE:
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                boxes.append([x, y, int(width), int(height)])
                centroids.append((centerX, centerY))
                confidences.append(float(confidence))

    idzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)

    if len(idzs) > 0:
        for i in idzs.flatten():
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            res = (confidences[i], (x, y, x + w, y + h), centroids[i])
            results.append(res)

    return results

# Write the Streamlit app to a file
streamlit_code = """
import streamlit as st
import numpy as np
import cv2
import imutils
import os

NMS_THRESHOLD = 0.3
MIN_CONFIDENCE = 0.2

def pedestrian_detection(image, model, layer_name, personidz=0):
    (H, W) = image.shape[:2]
    results = []

    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)
    model.setInput(blob)
    layerOutputs = model.forward(layer_name)

    boxes = []
    centroids = []
    confidences = []

    for output in layerOutputs:
        for detection in output:
            scores = detection[5:]
            classID = np.argmax(scores)
            confidence = scores[classID]

            if classID == personidz and confidence > MIN_CONFIDENCE:
                box = detection[0:4] * np.array([W, H, W, H])
                (centerX, centerY, width, height) = box.astype("int")
                x = int(centerX - (width / 2))
                y = int(centerY - (height / 2))

                boxes.append([x, y, int(width), int(height)])
                centroids.append((centerX, centerY))
                confidences.append(float(confidence))

    idzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)

    if len(idzs) > 0:
        for i in idzs.flatten():
            (x, y) = (boxes[i][0], boxes[i][1])
            (w, h) = (boxes[i][2], boxes[i][3])

            res = (confidences[i], (x, y, x + w, y + h), centroids[i])
            results.append(res)

    return results

# Load YOLO model
@st.cache_resource
def load_yolo_model():
    labelsPath = "coco.names"
    weights_path = "yolov4-tiny.weights"
    config_path = "yolov4-tiny.cfg"

    model = cv2.dnn.readNetFromDarknet(config_path, weights_path)
    layer_names = model.getLayerNames()
    layer_name = [layer_names[i[0] - 1] for i in model.getUnconnectedOutLayers()]

    labels = open(labelsPath).read().strip().split("\\n")
    return model, layer_name, labels

model, layer_name, labels = load_yolo_model()

st.title("Pedestrian Detection App")
st.write("Upload a video to detect pedestrians using YOLOv4-Tiny.")

uploaded_file = st.file_uploader("Choose a video file", type=["mp4", "avi", "mov"])

if uploaded_file is not None:
    video_path = f"uploaded_video.mp4"
    with open(video_path, "wb") as f:
        f.write(uploaded_file.read())

    cap = cv2.VideoCapture(video_path)
    output_video_path = "output_pedestrian_detection.avi"
    writer = None

    stframe = st.empty()

    while True:
        grabbed, image = cap.read()
        if not grabbed:
            break

        image = imutils.resize(image, width=700)
        results = pedestrian_detection(image, model, layer_name, personidz=labels.index("person"))

        for res in results:
            cv2.rectangle(image, (res[1][0], res[1][1]), (res[1][2], res[1][3]), (0, 255, 0), 2)

        if writer is None:
            fourcc = cv2.VideoWriter_fourcc(*"MJPG")
            writer = cv2.VideoWriter(output_video_path, fourcc, 30,
                                     (image.shape[1], image.shape[0]), True)

        writer.write(image)

        stframe.image(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), channels="RGB")

    cap.release()
    writer.release()

    st.success("Video processing complete! Click below to download the output video.")
    with open(output_video_path, "rb") as video_file:
        st.download_button("Download Processed Video", video_file, file_name="output_pedestrian_detection.avi")
"""

with open("app.py", "w") as f:
    f.write(streamlit_code)

# Start ngrok to expose the Streamlit app
public_url = ngrok.connect(8501)
print(f"Streamlit app is live at {public_url}")

# Run the Streamlit app
!streamlit run app.py &>/dev/null &

